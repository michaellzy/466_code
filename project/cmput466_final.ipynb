{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLqLG-3wtqtQ",
        "outputId": "5dcef972-7cc3-44d2-e933-a68da6c4515e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def predict(X, w, y=None):\n",
        "    # X_new: Nsample x (d+1)\n",
        "    # w: (d+1) x 1\n",
        "    # y_new: Nsample\n",
        "\n",
        "    z = np.dot(X, w)\n",
        "    y_hat = sigmoid(z)\n",
        "    loss = calc_loss_function(y_hat, y)\n",
        "    pred = np.round(y_hat)\n",
        "    acc = calc_accuracy(pred, y)\n",
        "    return y_hat, loss, acc\n",
        "\n",
        "\n",
        "def calc_accuracy(predicted_labels, actual_labels):\n",
        "    diff = predicted_labels - actual_labels\n",
        "    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))\n",
        "\n",
        "\n",
        "def calc_loss_function(y_hat, t):\n",
        "    N = t.shape[0]\n",
        "    cost1 = -t * np.log(y_hat)\n",
        "    cost2 = (1 - t) * np.log(1 - y_hat)\n",
        "    loss = cost1 - cost2\n",
        "    loss = loss.sum() / N\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train(X_train, y_train, X_val, t_val):\n",
        "    N_train = X_train.shape[0]\n",
        "    N_val = X_val.shape[0]\n",
        "\n",
        "    w = np.zeros((X_train.shape[1], N_class))\n",
        "\n",
        "    # w: (d+1)x1\n",
        "    acc_val_list = []\n",
        "    losses_train = []\n",
        "    W_best = None\n",
        "    acc_best = 0.0\n",
        "    epoch_best = 0\n",
        "    epoch_list = [i for i in range(1, MaxEpoch + 1)]\n",
        "\n",
        "    for epoch in range(MaxEpoch):\n",
        "        loss_this_epoch = 0\n",
        "        for b in range(int(np.ceil(N_train / batch_size))):\n",
        "            X_batch = X_train[b * batch_size: (b + 1) * batch_size]\n",
        "            y_batch = y_train[b * batch_size: (b + 1) * batch_size]\n",
        "\n",
        "            y_hat_batch, loss_batch, acc = predict(X_batch, w, y_batch)\n",
        "            loss_this_epoch += loss_batch\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            w = w - alpha * (1 / batch_size) * np.dot(X_batch.T, y_hat_batch - y_batch)\n",
        "\n",
        "        # monitor model behavior after each epoch\n",
        "        # 1. Compute the training loss by averaging loss_this_epoc\n",
        "        # loss_this_epoch = loss_this_epoch / int(np.ceil(N_train / batch_size))\n",
        "        loss_this_epoch = loss_this_epoch / N_train\n",
        "        losses_train.append(loss_this_epoch)\n",
        "        # print(loss_this_epoch)\n",
        "        # 2.perform validation on the validation dataset\n",
        "        _, _, acc_val = predict(X_val, w, t_val)\n",
        "        acc_val_list.append(acc_val)\n",
        "        # 3. keep track of the best validation epoch, acc, and weight\n",
        "        current_acc_best = max(acc_best, acc_val)\n",
        "        if current_acc_best != acc_best:\n",
        "            acc_best = current_acc_best\n",
        "            W_best = w\n",
        "            epoch_best = epoch\n",
        "        print(\n",
        "            \"epoch:[{0}/{1}]\\t\"\n",
        "            \"alpha:{2}\\t\"\n",
        "            \"train loss: {loss:.5f}\\t\"\n",
        "            \"acc:{acc:.5f}\\t\"\n",
        "            \"acc best:{acc_best:.5f}\\t\"\n",
        "            \"epoch best:{epoch_best}\\t\".format(epoch, MaxEpoch, alpha, loss=loss_this_epoch, acc=acc_val, acc_best=acc_best,\n",
        "                                               epoch_best=epoch_best))\n",
        "\n",
        "    # plot_train_graph(losses_train, epoch_list)\n",
        "    # plot_val_graph(acc_val_list, epoch_list)\n",
        "    return epoch_best, acc_best, W_best\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # alpha = 0.1  # learning rate\n",
        "    batch_size = 100  # batch size\n",
        "    # MaxEpoch = 50  # Maximum epoch\n",
        "    alpha_list = [0.1, 0.01]\n",
        "    MaxEpoch_list = [20, 50]\n",
        "    N_class = 1\n",
        "\n",
        "    data = pd.read_csv(\"/content/drive/My Drive/cmput466_final/spambase/spambase.csv\")\n",
        "    target_arr = []\n",
        "    data_target = data[\"class\"]\n",
        "\n",
        "    for ele in data_target.items():\n",
        "        target_arr.append(ele[1])\n",
        "    y_ = np.array(target_arr)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :-1], y_, test_size=0.2, random_state=0)\n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "    x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
        "    x_val = (x_val - np.mean(x_val, axis=0)) / np.std(x_val, axis=0)\n",
        "    x_test = (x_test - np.mean(x_test, axis=0)) / np.std(x_test, axis=0)\n",
        "\n",
        "    X_train = np.concatenate((np.ones([x_train.shape[0], 1]), x_train), axis=1)\n",
        "    X_val = np.concatenate((np.ones([x_val.shape[0], 1]), x_val), axis=1)\n",
        "    X_test = np.concatenate((np.ones([x_test.shape[0], 1]), x_test), axis=1)\n",
        "\n",
        "    # print(X_train.shape)\n",
        "    # print(X_val.shape)\n",
        "    # print(X_test.shape)\n",
        "\n",
        "    y_train = y_train.reshape([-1, 1])\n",
        "    y_val = y_val.reshape([-1, 1])\n",
        "    y_test = y_test.reshape([-1, 1])\n",
        "\n",
        "    for alpha in alpha_list:\n",
        "        for MaxEpoch in MaxEpoch_list:\n",
        "            _, _, w = train(X_train, y_train, X_val, y_val)\n",
        "            y_hat_test, _, acc_test = predict(X_test, w, y_test)\n",
        "            print(\"test accuracy : %.5f\" % acc_test)\n",
        "            print(\"---------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6PeITBnjdSb",
        "outputId": "4fffe693-1dfb-4642-d196-34823d32ee92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:[0/20]\talpha:0.1\ttrain loss: 0.00463\tacc:0.90109\tacc best:0.90109\tepoch best:0\t\n",
            "epoch:[1/20]\talpha:0.1\ttrain loss: 0.00332\tacc:0.90326\tacc best:0.90326\tepoch best:1\t\n",
            "epoch:[2/20]\talpha:0.1\ttrain loss: 0.00299\tacc:0.90652\tacc best:0.90652\tepoch best:2\t\n",
            "epoch:[3/20]\talpha:0.1\ttrain loss: 0.00283\tacc:0.90978\tacc best:0.90978\tepoch best:3\t\n",
            "epoch:[4/20]\talpha:0.1\ttrain loss: 0.00272\tacc:0.90870\tacc best:0.90978\tepoch best:3\t\n",
            "epoch:[5/20]\talpha:0.1\ttrain loss: 0.00265\tacc:0.91087\tacc best:0.91087\tepoch best:5\t\n",
            "epoch:[6/20]\talpha:0.1\ttrain loss: 0.00260\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[7/20]\talpha:0.1\ttrain loss: 0.00255\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[8/20]\talpha:0.1\ttrain loss: 0.00252\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[9/20]\talpha:0.1\ttrain loss: 0.00248\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[10/20]\talpha:0.1\ttrain loss: 0.00246\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[11/20]\talpha:0.1\ttrain loss: 0.00243\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[12/20]\talpha:0.1\ttrain loss: 0.00241\tacc:0.91087\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[13/20]\talpha:0.1\ttrain loss: 0.00239\tacc:0.91087\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[14/20]\talpha:0.1\ttrain loss: 0.00238\tacc:0.91087\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[15/20]\talpha:0.1\ttrain loss: 0.00236\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[16/20]\talpha:0.1\ttrain loss: 0.00235\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[17/20]\talpha:0.1\ttrain loss: 0.00233\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[18/20]\talpha:0.1\ttrain loss: 0.00232\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[19/20]\talpha:0.1\ttrain loss: 0.00231\tacc:0.91196\tacc best:0.91304\tepoch best:6\t\n",
            "test accuracy : 0.90554\n",
            "---------------------------------------------\n",
            "epoch:[0/50]\talpha:0.1\ttrain loss: 0.00463\tacc:0.90109\tacc best:0.90109\tepoch best:0\t\n",
            "epoch:[1/50]\talpha:0.1\ttrain loss: 0.00332\tacc:0.90326\tacc best:0.90326\tepoch best:1\t\n",
            "epoch:[2/50]\talpha:0.1\ttrain loss: 0.00299\tacc:0.90652\tacc best:0.90652\tepoch best:2\t\n",
            "epoch:[3/50]\talpha:0.1\ttrain loss: 0.00283\tacc:0.90978\tacc best:0.90978\tepoch best:3\t\n",
            "epoch:[4/50]\talpha:0.1\ttrain loss: 0.00272\tacc:0.90870\tacc best:0.90978\tepoch best:3\t\n",
            "epoch:[5/50]\talpha:0.1\ttrain loss: 0.00265\tacc:0.91087\tacc best:0.91087\tepoch best:5\t\n",
            "epoch:[6/50]\talpha:0.1\ttrain loss: 0.00260\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[7/50]\talpha:0.1\ttrain loss: 0.00255\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:[8/50]\talpha:0.1\ttrain loss: 0.00252\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[9/50]\talpha:0.1\ttrain loss: 0.00248\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[10/50]\talpha:0.1\ttrain loss: 0.00246\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[11/50]\talpha:0.1\ttrain loss: 0.00243\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[12/50]\talpha:0.1\ttrain loss: 0.00241\tacc:0.91087\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[13/50]\talpha:0.1\ttrain loss: 0.00239\tacc:0.91087\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[14/50]\talpha:0.1\ttrain loss: 0.00238\tacc:0.91087\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[15/50]\talpha:0.1\ttrain loss: 0.00236\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[16/50]\talpha:0.1\ttrain loss: 0.00235\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[17/50]\talpha:0.1\ttrain loss: 0.00233\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[18/50]\talpha:0.1\ttrain loss: 0.00232\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[19/50]\talpha:0.1\ttrain loss: 0.00231\tacc:0.91196\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[20/50]\talpha:0.1\ttrain loss: 0.00230\tacc:0.91196\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[21/50]\talpha:0.1\ttrain loss: 0.00229\tacc:0.91196\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[22/50]\talpha:0.1\ttrain loss: 0.00228\tacc:0.91196\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[23/50]\talpha:0.1\ttrain loss: 0.00227\tacc:0.91196\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[24/50]\talpha:0.1\ttrain loss: 0.00226\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[25/50]\talpha:0.1\ttrain loss: 0.00226\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[26/50]\talpha:0.1\ttrain loss: 0.00225\tacc:0.91304\tacc best:0.91304\tepoch best:6\t\n",
            "epoch:[27/50]\talpha:0.1\ttrain loss: 0.00224\tacc:0.91413\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[28/50]\talpha:0.1\ttrain loss: 0.00224\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[29/50]\talpha:0.1\ttrain loss: 0.00223\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[30/50]\talpha:0.1\ttrain loss: 0.00222\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[31/50]\talpha:0.1\ttrain loss: 0.00222\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[32/50]\talpha:0.1\ttrain loss: 0.00221\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[33/50]\talpha:0.1\ttrain loss: 0.00221\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[34/50]\talpha:0.1\ttrain loss: 0.00220\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[35/50]\talpha:0.1\ttrain loss: 0.00220\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[36/50]\talpha:0.1\ttrain loss: 0.00219\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[37/50]\talpha:0.1\ttrain loss: 0.00219\tacc:0.91196\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[38/50]\talpha:0.1\ttrain loss: 0.00219\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[39/50]\talpha:0.1\ttrain loss: 0.00218\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[40/50]\talpha:0.1\ttrain loss: 0.00218\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[41/50]\talpha:0.1\ttrain loss: 0.00218\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[42/50]\talpha:0.1\ttrain loss: 0.00217\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[43/50]\talpha:0.1\ttrain loss: 0.00217\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[44/50]\talpha:0.1\ttrain loss: 0.00216\tacc:0.91413\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[45/50]\talpha:0.1\ttrain loss: 0.00216\tacc:0.91413\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[46/50]\talpha:0.1\ttrain loss: 0.00216\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[47/50]\talpha:0.1\ttrain loss: 0.00216\tacc:0.91304\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[48/50]\talpha:0.1\ttrain loss: 0.00215\tacc:0.91196\tacc best:0.91413\tepoch best:27\t\n",
            "epoch:[49/50]\talpha:0.1\ttrain loss: 0.00215\tacc:0.91196\tacc best:0.91413\tepoch best:27\t\n",
            "test accuracy : 0.91531\n",
            "---------------------------------------------\n",
            "epoch:[0/20]\talpha:0.01\ttrain loss: 0.00647\tacc:0.89565\tacc best:0.89565\tepoch best:0\t\n",
            "epoch:[1/20]\talpha:0.01\ttrain loss: 0.00563\tacc:0.89565\tacc best:0.89565\tepoch best:0\t\n",
            "epoch:[2/20]\talpha:0.01\ttrain loss: 0.00508\tacc:0.89783\tacc best:0.89783\tepoch best:2\t\n",
            "epoch:[3/20]\talpha:0.01\ttrain loss: 0.00470\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[4/20]\talpha:0.01\ttrain loss: 0.00441\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[5/20]\talpha:0.01\ttrain loss: 0.00419\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[6/20]\talpha:0.01\ttrain loss: 0.00402\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[7/20]\talpha:0.01\ttrain loss: 0.00387\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[8/20]\talpha:0.01\ttrain loss: 0.00375\tacc:0.89891\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[9/20]\talpha:0.01\ttrain loss: 0.00365\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[10/20]\talpha:0.01\ttrain loss: 0.00356\tacc:0.90109\tacc best:0.90109\tepoch best:10\t\n",
            "epoch:[11/20]\talpha:0.01\ttrain loss: 0.00349\tacc:0.90109\tacc best:0.90109\tepoch best:10\t\n",
            "epoch:[12/20]\talpha:0.01\ttrain loss: 0.00342\tacc:0.90109\tacc best:0.90109\tepoch best:10\t\n",
            "epoch:[13/20]\talpha:0.01\ttrain loss: 0.00336\tacc:0.90217\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[14/20]\talpha:0.01\ttrain loss: 0.00331\tacc:0.90109\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[15/20]\talpha:0.01\ttrain loss: 0.00326\tacc:0.90109\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[16/20]\talpha:0.01\ttrain loss: 0.00322\tacc:0.90109\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[17/20]\talpha:0.01\ttrain loss: 0.00318\tacc:0.90217\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[18/20]\talpha:0.01\ttrain loss: 0.00314\tacc:0.90326\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[19/20]\talpha:0.01\ttrain loss: 0.00311\tacc:0.90326\tacc best:0.90326\tepoch best:18\t\n",
            "test accuracy : 0.90554\n",
            "---------------------------------------------\n",
            "epoch:[0/50]\talpha:0.01\ttrain loss: 0.00647\tacc:0.89565\tacc best:0.89565\tepoch best:0\t\n",
            "epoch:[1/50]\talpha:0.01\ttrain loss: 0.00563\tacc:0.89565\tacc best:0.89565\tepoch best:0\t\n",
            "epoch:[2/50]\talpha:0.01\ttrain loss: 0.00508\tacc:0.89783\tacc best:0.89783\tepoch best:2\t\n",
            "epoch:[3/50]\talpha:0.01\ttrain loss: 0.00470\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[4/50]\talpha:0.01\ttrain loss: 0.00441\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[5/50]\talpha:0.01\ttrain loss: 0.00419\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[6/50]\talpha:0.01\ttrain loss: 0.00402\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[7/50]\talpha:0.01\ttrain loss: 0.00387\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[8/50]\talpha:0.01\ttrain loss: 0.00375\tacc:0.89891\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[9/50]\talpha:0.01\ttrain loss: 0.00365\tacc:0.90000\tacc best:0.90000\tepoch best:3\t\n",
            "epoch:[10/50]\talpha:0.01\ttrain loss: 0.00356\tacc:0.90109\tacc best:0.90109\tepoch best:10\t\n",
            "epoch:[11/50]\talpha:0.01\ttrain loss: 0.00349\tacc:0.90109\tacc best:0.90109\tepoch best:10\t\n",
            "epoch:[12/50]\talpha:0.01\ttrain loss: 0.00342\tacc:0.90109\tacc best:0.90109\tepoch best:10\t\n",
            "epoch:[13/50]\talpha:0.01\ttrain loss: 0.00336\tacc:0.90217\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[14/50]\talpha:0.01\ttrain loss: 0.00331\tacc:0.90109\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[15/50]\talpha:0.01\ttrain loss: 0.00326\tacc:0.90109\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[16/50]\talpha:0.01\ttrain loss: 0.00322\tacc:0.90109\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[17/50]\talpha:0.01\ttrain loss: 0.00318\tacc:0.90217\tacc best:0.90217\tepoch best:13\t\n",
            "epoch:[18/50]\talpha:0.01\ttrain loss: 0.00314\tacc:0.90326\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[19/50]\talpha:0.01\ttrain loss: 0.00311\tacc:0.90326\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[20/50]\talpha:0.01\ttrain loss: 0.00308\tacc:0.90326\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[21/50]\talpha:0.01\ttrain loss: 0.00305\tacc:0.90217\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[22/50]\talpha:0.01\ttrain loss: 0.00303\tacc:0.90217\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[23/50]\talpha:0.01\ttrain loss: 0.00300\tacc:0.90326\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[24/50]\talpha:0.01\ttrain loss: 0.00298\tacc:0.90326\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[25/50]\talpha:0.01\ttrain loss: 0.00296\tacc:0.90326\tacc best:0.90326\tepoch best:18\t\n",
            "epoch:[26/50]\talpha:0.01\ttrain loss: 0.00294\tacc:0.90543\tacc best:0.90543\tepoch best:26\t\n",
            "epoch:[27/50]\talpha:0.01\ttrain loss: 0.00292\tacc:0.90543\tacc best:0.90543\tepoch best:26\t\n",
            "epoch:[28/50]\talpha:0.01\ttrain loss: 0.00290\tacc:0.90652\tacc best:0.90652\tepoch best:28\t\n",
            "epoch:[29/50]\talpha:0.01\ttrain loss: 0.00289\tacc:0.90652\tacc best:0.90652\tepoch best:28\t\n",
            "epoch:[30/50]\talpha:0.01\ttrain loss: 0.00287\tacc:0.90761\tacc best:0.90761\tepoch best:30\t\n",
            "epoch:[31/50]\talpha:0.01\ttrain loss: 0.00286\tacc:0.90761\tacc best:0.90761\tepoch best:30\t\n",
            "epoch:[32/50]\talpha:0.01\ttrain loss: 0.00284\tacc:0.90761\tacc best:0.90761\tepoch best:30\t\n",
            "epoch:[33/50]\talpha:0.01\ttrain loss: 0.00283\tacc:0.90761\tacc best:0.90761\tepoch best:30\t\n",
            "epoch:[34/50]\talpha:0.01\ttrain loss: 0.00282\tacc:0.90761\tacc best:0.90761\tepoch best:30\t\n",
            "epoch:[35/50]\talpha:0.01\ttrain loss: 0.00280\tacc:0.90761\tacc best:0.90761\tepoch best:30\t\n",
            "epoch:[36/50]\talpha:0.01\ttrain loss: 0.00279\tacc:0.90870\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[37/50]\talpha:0.01\ttrain loss: 0.00278\tacc:0.90870\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[38/50]\talpha:0.01\ttrain loss: 0.00277\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[39/50]\talpha:0.01\ttrain loss: 0.00276\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[40/50]\talpha:0.01\ttrain loss: 0.00275\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[41/50]\talpha:0.01\ttrain loss: 0.00274\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[42/50]\talpha:0.01\ttrain loss: 0.00273\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[43/50]\talpha:0.01\ttrain loss: 0.00272\tacc:0.90870\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[44/50]\talpha:0.01\ttrain loss: 0.00271\tacc:0.90870\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[45/50]\talpha:0.01\ttrain loss: 0.00271\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[46/50]\talpha:0.01\ttrain loss: 0.00270\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[47/50]\talpha:0.01\ttrain loss: 0.00269\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[48/50]\talpha:0.01\ttrain loss: 0.00268\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "epoch:[49/50]\talpha:0.01\ttrain loss: 0.00267\tacc:0.90761\tacc best:0.90870\tepoch best:36\t\n",
            "test accuracy : 0.90337\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "def data_preprocess():\n",
        "    data = pd.read_csv(\"/content/drive/My Drive/cmput466_final/spambase/spambase.csv\")\n",
        "    target_arr = []\n",
        "    data_target = data[\"class\"]\n",
        "\n",
        "    for ele in data_target.items():\n",
        "        target_arr.append(ele[1])\n",
        "    y_ = np.array(target_arr)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :-1], y_, test_size=0.2, random_state=0)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "\n",
        "    x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
        "    x_test = (x_test - np.mean(x_test, axis=0)) / np.std(x_test, axis=0)\n",
        "\n",
        "    X_train = np.concatenate((np.ones([x_train.shape[0], 1]), x_train), axis=1)\n",
        "    X_test = np.concatenate((np.ones([x_test.shape[0], 1]), x_test), axis=1)\n",
        "\n",
        "    y_train = y_train.reshape([-1, 1])\n",
        "    y_test = y_test.reshape([-1, 1])\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def calc_accuracy(predicted_labels, actual_labels):\n",
        "    diff = predicted_labels - actual_labels\n",
        "    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))\n",
        "\n",
        "def majority_guess(y_train, y_test):\n",
        "    prediction = np.bincount(y_train.flatten()).argmax()\n",
        "    guess = np.array([prediction for i in range(len(y_test))])\n",
        "    guess = guess.reshape([-1, 1])\n",
        "    return guess\n",
        "\n",
        "def decision_tree(X_train, y_train, X_test):\n",
        "    classifier = DecisionTreeClassifier()\n",
        "    classifier.fit(X_train, y_train)\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    y_pred = y_pred.reshape([-1, 1])\n",
        "    return y_pred\n",
        "\n",
        "def knn(X_train, y_train, X_test, k):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    y_pred = y_pred.reshape([-1, 1])\n",
        "    return y_pred\n",
        "\n",
        "def neural_network(X_train, y_train, X_test, opt, max_iter):\n",
        "    nn = MLPClassifier(solver=opt, activation='relu', alpha=1e-3, hidden_layer_sizes = (5, 2), max_iter=max_iter)\n",
        "    nn.fit(X_train, y_train)\n",
        "    y_pred = nn.predict(X_test)\n",
        "    y_pred = y_pred.reshape([-1, 1])\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    X_train, y_train, X_test, y_test = data_preprocess()\n",
        "    print(X_train.shape)\n",
        "    print(y_train.shape)\n",
        "    print(X_test.shape)\n",
        "    print(y_test.shape)\n",
        "    maj_guess = majority_guess(y_train, y_test)\n",
        "    acc_maj = calc_accuracy(maj_guess, y_test)\n",
        "    print(\"Acc of majority guess: %.5f\" % acc_maj)\n",
        "\n",
        "    dt = decision_tree(X_train, y_train.flatten(), X_test)\n",
        "    acc_nb = calc_accuracy(dt, y_test)\n",
        "    print(\"Acc of decision tree: %.5f\" % acc_nb)\n",
        "\n",
        "    for k in range(3, 6):\n",
        "      knn_res = knn(X_train, y_train.flatten(), X_test, k)\n",
        "      acc_knn = calc_accuracy(knn_res, y_test)\n",
        "      print(\"Acc of knn: %.5f with k = %d\" % (acc_knn, k))\n",
        "    for opt in [\"adam\", \"sgd\"]:\n",
        "      for max_iter in [100, 200]:\n",
        "        nn = neural_network(X_train, y_train.flatten(), X_test, opt=opt, max_iter=max_iter)\n",
        "        acc_nn = calc_accuracy(nn, y_test)\n",
        "        print(\"iteration:[%d], opt:[%s], Acc of neural network: %.5f\" % (max_iter, opt, acc_nn))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKNizcgLjNYk",
        "outputId": "fe919b16-0c90-4381-97db-b5548ce597c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2760, 58)\n",
            "(2760, 1)\n",
            "(921, 58)\n",
            "(921, 1)\n",
            "Acc of majority guess: 0.58415\n",
            "Acc of decision tree: 0.91857\n",
            "Acc of knn: 0.91531 with k = 3\n",
            "Acc of knn: 0.90988 with k = 4\n",
            "Acc of knn: 0.91314 with k = 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:[100], opt:[adam], Acc of neural network: 0.91965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:[200], opt:[adam], Acc of neural network: 0.91314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:[100], opt:[sgd], Acc of neural network: 0.90445\n",
            "iteration:[200], opt:[sgd], Acc of neural network: 0.89142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    }
  ]
}